# -*- coding: utf-8 -*-
"""HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yv3SQNvXEJsXBTOy81WhZ6_oFbdF_g8E

# Homework 3
Written February 2023 by Thomas Pinkava
"""


import pandas as pd

# Read csv: Train only, of course
# Data encodes NaNs as '-1', so pick those up while parsing.
data_raw = pd.read_csv('train.csv', na_values = ["-1"])

X = data_raw.drop(['id'], axis = 1)

"""# Question 1

## Data Exploration and Cleaning
*From the data source on Kaggle:*

> In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features. Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder.

So we can establish column types with the following code:
"""

from sklearn.model_selection import train_test_split
# Establish column types
non_target_columns = [col for col in X.columns if col != "target"]
categorical_features = [column_candidate for column_candidate in non_target_columns if "_cat" in column_candidate]
numeric_features = [column_candidate for column_candidate in non_target_columns if column_candidate not in categorical_features]


# Data Exploration
#X.columns
#for column in X.columns:
	#print(column + ":\t" + str(X[column].isna().sum() / len(X[column])))
  #print(X[column].value_counts())


# Preprocessing pipeline
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

num_preproc_pipeline = Pipeline(steps=[
    ('imputation', SimpleImputer()),
    ('scaling', StandardScaler())
])

cat_preproc_pipeline = Pipeline(steps=[
    ('imputation', SimpleImputer()),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers = [
        ('num_preproc', num_preproc_pipeline, numeric_features),
        ('cat_preproc', cat_preproc_pipeline, categorical_features),
    ]
)


# Perform train/test split and preprocess data
from sklearn.model_selection import train_test_split

y = X['target'] # y is already binary; no need to scale; no reason to impute.
X = X.drop(['target'], axis=1)



# Perform the train-test split here, as the imputer needs to be fit, and it would
# be wrong to fit it on test data.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)
X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)



"""## Generating the rebalanced data"""

# Randomly undersampled: drop whatever
from imblearn.under_sampling import RandomUnderSampler
undersampler = RandomUnderSampler(random_state=1)
X_undersampled, y_undersampled = undersampler.fit_resample(X_train, y_train)


# Randomly oversampled: sample-with-replacement
from imblearn.over_sampling import RandomOverSampler
oversampler = RandomOverSampler(random_state=1)
X_oversampled, y_oversampled = oversampler.fit_resample(X_train, y_train)


# Tomek Links: drop the frontiersmen
from imblearn.under_sampling import TomekLinks
tomek_sampler = TomekLinks()
X_tomek, y_tomek = tomek_sampler.fit_resample(X_train, y_train) # Whoever "optimized" the pairwise euclidean comparison should be banned from computers


# SMOTE: synthesize new minorities inside the minority cluster
from imblearn.over_sampling import SMOTE
SMOTE_sampler = SMOTE(random_state=1)
X_SMOTE, y_SMOTE = SMOTE_sampler.fit_resample(X_train, y_train)


# Assemble a list of datasets
datasets = [
    ("Unrebalanced", X_train, y_train),
    ("Randomly undersampled", X_undersampled, y_undersampled),
    ("Randomly oversampled", X_oversampled, y_oversampled),
    ("Tomek Links", X_tomek, y_tomek),
    ("SMOTE", X_SMOTE, y_SMOTE)
]

"""## Classifiers"""

# Make an output list for automatic testing and output
classifiers = []

# Logistic Regression
from sklearn.linear_model import LogisticRegression
for data_name, data_X, data_y in datasets:
  clf = LogisticRegression(random_state=1)
  clf.fit(data_X, data_y)
  classifiers.append(("Logistic Regression", data_name, clf))


# Naive Bayes (Gaussian, I suppose)
from sklearn.naive_bayes import GaussianNB
for data_name, data_X, data_y in datasets:
  clf = GaussianNB()
  clf.fit(data_X, data_y)                 # N.B. Gaussian needs dense; use "toarray" if it went sparse
  classifiers.append(("Gaussian", data_name, clf))


# Random Forest
from sklearn.ensemble import RandomForestClassifier
for data_name, data_X, data_y in datasets:
  clf = RandomForestClassifier(random_state = 1)
  clf.fit(data_X, data_y)
  classifiers.append(("Random Forest", data_name, clf))


# Balanced Random Forest
from imblearn.ensemble import BalancedRandomForestClassifier
clf = BalancedRandomForestClassifier(random_state = 1)
clf.fit(X_train, y_train)
classifiers.append(("Balanced Random Forest", "Unrebalanced", clf))


# Adaboost with undersampling
from imblearn.ensemble import RUSBoostClassifier
clf = RUSBoostClassifier(random_state = 1)
clf.fit(X_train, y_train)
classifiers.append(("Adaboost with random undersampling", "Unrebalanced", clf))


# Gradient Boost
from sklearn.ensemble import GradientBoostingClassifier
clf = GradientBoostingClassifier(random_state = 1)
clf.fit(X_train, y_train)
classifiers.append(("Gradient Boost", "Unrebalanced", clf))

# Test all classifiers and report summary statistics
from sklearn.metrics import precision_score, recall_score, roc_auc_score

results = []

best_roc_auc = 0.0
best_model = "Zilch"

for classifier in classifiers:
  clf = classifier[2]

  # Generate predictions
  y_pred = clf.predict(X_test)

  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  f1 = 2 * (precision * recall) / (precision + recall)  # Save the computer some effort. It deserves it, poor thing, after the Tomek Links.

  # ROC AUC needs probabilities, not predictions:
  roc_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])
  if(roc_auc > best_roc_auc):
    best_roc_auc = roc_auc
    best_model = classifier[0]

  # Add the results to list
  results.append((classifier[0], classifier[1], precision, recall, f1, roc_auc))


print(classifiers)

# Print summary
for result in results:
  print("{} on {} dataset:\n\tPrecision: {}\n\tRecall: {}\n\tF1 Score: {}\n\tROC AUC: {}\n".format(*result))

print("Best Classifier (AUC): {}".format(best_model))



"""# Question 2: Ensemble Classifier

TODO
"""
