# -*- coding: utf-8 -*-
"""final_project_NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1urV7EiKjkqUqstbf0u2mx1XvxlR64l4C

# CSS 581 Final Project

Thomas Pinkava, Abdul-Muizz Imtiaz

---

### Dataset: User->Movie Ratings
Data provenance: [Grouplens Project](https://grouplens.org/datasets/movielens/latest/), University of Minnesota

F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. *ACM Transactions on Interactive Intelligent Systems (TiiS)* 5, 4: 19:1â€“19:19. <https://doi.org/10.1145/2827872>

**Input Data file format**

| Datafile          | Content 1 | Content 2   | Content 3  | Content 4 |
|-------------------|-----------|-------------|------------|-----------|
| movies.csv        | movie ID  | movie title | genre list |           |
| ratings.csv       | user ID   | movie ID    | rating     | timestamp |
| tags.csv          | user ID   | movie ID    | tag text   | timestamp |
| genome_scores.csv | movie ID  | tag ID      | relevance  |           |
| genome_tags.csv   | tag ID    | tag text    |            |           |
"""

# Mount drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

"""## Hybrid Prototype: Content filter (dataclean)

**Output Data file format**

TODO something like User ID, Movie Title, Genre onehots, tag text sample

tags_raw = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/tags.csv')

for n in range(1, 800, 20):
  tags_with_more_than_n_occurences = []
  for tag, count in tags_raw.tag.value_counts().items():
    if count > n:
      tags_with_more_than_n_occurences.append((tag, count))

  print("{} tags with more than {} occurences".format(len(tags_with_more_than_n_occurences), n))

# Load datasets
#'/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/ratings.csv'
#'/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/genome-scores.csv'
#'/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/genome-tags.csv'
#'/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/movies.csv'
#'/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/tags.csv'

import pandas as pd

#ratings_raw = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/ratings.csv')
movies_raw = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/movies.csv')

# Process movie database to generate one-hot genre encodings (this list contains all possible dataset genre values)
all_genres = ["Action","Adventure","Animation","Children","Comedy","Crime","Documentary","Drama","Fantasy","Film-Noir","Horror","Musical","Mystery","Romance","Sci-Fi","Thriller","War","Western"]

def contains_multihot_encoder(dataframe, feature, containables):
  for containable in containables:
    dataframe[feature + "_" + containable] = dataframe[feature].apply(lambda f: 1.0 if containable in f else 0.0)
  return dataframe.drop(feature, axis=1)

movies_raw = contains_multihot_encoder(movies_raw, 'genres', all_genres)


# Collate user rating and movie databases: redundantify information
# TODO

#del ratings_raw
#del movies_raw

## Nearest Neighbour Collaborative Filtering
"""

# Ways to store our dataset in memory:
# Approach 1: 2d matrix where rows are users and columns are films. Each cell is a user rating for that film
# Approach 2: list of dictionaries such as: [{"movieID1": 5, "movieID3": 10, "movieID5", 6},   {"movieID": 2}]

# We will use approach 2 because its saves memory space (since we don't store movies that user didn't see with null values)
# NOTE: Our algorithm works with both approaches since we will either ignore nulls or we impute them somehow

import pandas as pd
import numpy as np

ratings_raw = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/ratings.csv')

ratings_raw.head()

#Checking if userId is in ascending order (which it is)
# value = ratings_raw["userId"].is_monotonic_increasing
# print(value)

# print(ratings_raw["userId"].nunique(axis=0))

"""#### Input File Parsing"""

users = []

with open("/content/drive/MyDrive/Colab Notebooks/581 Final Project/data_raw/ratings.csv", mode = "r") as ratingsFile:
  next(ratingsFile) #skip first line
  dictionary = {}

  previousUserID = 1 # Initial user ID in file; nongeneralizable
  for line in ratingsFile:
    elements = line.split(",") #this contains userID, movieID and rating
    userID = int(elements[0])
    movieID = int(elements[1])
    rating = int(float(elements[2]) * 2.0)  # Just for form's sake, switch from 0.0-5.0 to 0-10
    
    if (previousUserID != userID): # new user; write out old user to list and clear ground for new
      if (len(dictionary) > 1): 
        users.append(dictionary)
      dictionary = {}
    
    dictionary[movieID] = rating  
    previousUserID = int(elements[0])

original_users = users # for PRUNING BELOW

# PRUNING FOR SPEED
import random
users = original_users
random.shuffle(users)  
users = users[:len(users)//10] 
print("Downsampled to {} users".format(len(users)))

"""#### Data Exploration"""

if False: 
  # Number of films per user
  films_per_user = [len(user) for user in users]

  # Number of users per film
  users_per_film = {}
  for user in users:
    for film in user:
      if film in users_per_film:
        users_per_film[film] += 1
      else:
        users_per_film[film] = 1


  import matplotlib.pyplot as plt

  plt.hist(films_per_user)
  plt.yscale('log')
  plt.title('Histogram of films per reviewer')
  plt.xlabel('Films per reviewer')
  plt.show()

  plt.hist(users_per_film)
  plt.yscale('log')
  plt.title('Histogram of reviews per film')
  plt.xlabel('Reviews per film')
  plt.show()

"""#### K-Fold CV iterator class"""

# For convenience (and because scipy's sparse matrices are not what they purport to be)
# we define a custom iterator for our K-fold cross-validation.
import math 

class CustomKFoldIterator:
  def __init__(self, items, num_folds, test_fold_index):
    self.items = items
    self.test_fold_index = test_fold_index
    self.num_folds = num_folds
    self.fold_length = int(len(items) / num_folds)   # Drop the remaining observations

  
  # Generator for test users (iterates over indices in the test_fold_index fold)
  def test_users_iterator(self):
    test_fold_start = self.test_fold_index * self.fold_length
    for index in range(self.fold_length):
      yield self.items[index + test_fold_start]

  
  # Generator for train users (iterates over every non-test index)
  def train_users_iterator(self):
    for fold_index in range(self.num_folds):
      if not fold_index == self.test_fold_index:
        fold_start = fold_index * self.fold_length
        for index in range(self.fold_length):
          yield self.items[fold_start + index]

"""#### NN CF Algorithm"""

# Uncomment plot calls below and dependencies to print similarity histogram
import matplotlib.pyplot as plt

# Diagnostic helper function: prints percentage fill meter
def print_progress(part, total):
  bar_length = 20
  print("\r[{}] {}/{}".format(''.join(['=' if x < ((part/total) * bar_length) else ' ' for x in range(bar_length)]), part, total),  end='')

# I can't believe Python doesn't have a native linear interpolation function
def lerp(x, x0, x1, y0, y1):
  return y0 + (x - x0) * ((y1 - y0)/(x1 - x0))


# Define our train-test process as a function so we can tune hyperparameters
# Params:
#   similarity_threshold: absolute similarity value beneath which neighbours are not considered in estimation
#   coconsumption_factor: 0.0 means coconsumption does not affect similarity; 1.0 means similarity directly multiplied
#     by number of films in test user divided by number of films shared between test and train user
#   controversy_factor: 0.0 means controversy does not affect film; 1.0 means film directly weighted by rating variance, etc. 
#   k: -fold cross validation
#
# Returns:
#   Tuple containing:
#     Mean Squared Error of ratings on test set
#     Mean Absolute Error of ratings on test set
def train_test_nn_cf(similarity_threshold = 0.1, coconsumption_factor = 0.0, controversy_factor = 0.0,  k = 3):

  overall_mse = 0
  overall_mae = 0

  fold_count = 1  # Purely for diagnostic printing
  for test_fold_index in range(k):
    print("Fold {} of {}".format(fold_count, k))
    fold_count += 1
    k_folder = CustomKFoldIterator(users, k, test_fold_index)

    # For every fold:

    user_index = 0 # Purely for diagnostic printing
    # user_mean_similarities = [] # Data exploration

    fold_mse = 0
    fold_mae = 0

    # Compute controversies (to avoid data leakage)
    ratings = {}
    controversy = {}
    for user in k_folder.train_users_iterator():
      for film in user:
        if film in ratings:
          ratings[film].append(user[film])
        else:
          ratings[film] = [user[film]]
    for rating in ratings:
      controversy[rating] = np.var(ratings[rating])


    for test_user in k_folder.test_users_iterator():
      # For each test user in the test set

      # (print progress)
      user_index += 1
      print_progress(user_index, k_folder.fold_length)

      # 1) Split films into train and test (TODO make 80/20, not 50/50)
      test_user_train_films = dict(list(test_user.items())[len(test_user)//2:])
      test_user_test_films = dict(list(test_user.items())[:len(test_user)//2])

      # 2) Compute test user's average film rating
      test_user_avg_rating = sum(test_user_train_films.values()) / len(test_user_train_films) 


      # 3) Compute similarity scores (Pearson correlation) between test user and all train users
      #list_of_similarities = []  # Old model
      sufficiently_similar_others = []  # A refinement: similarity threshold allows us to prune unwanted users.
      for train_user in k_folder.train_users_iterator():
        numerator = 0
        leftDenominatorTerm = 0
        rightDenominatorTerm = 0
        
        # Compute train user average film rating (TODO precompute/cache?)
        train_user_avg_rating = sum(train_user.values()) / len(train_user)

        # Compute individual Pearson contributions
        intersection = [(test_user_train_films[x], train_user[x], controversy[x]) for x in test_user_train_films if x in train_user]
        for test_rating, train_rating, controversy_val in intersection:
          controversy_based_weight = lerp(controversy_factor, 0.0, 1.0, 1.0, controversy_val)
          m = (test_rating - test_user_avg_rating) 
          n = (train_rating * controversy_based_weight - train_user_avg_rating)
          numerator += m * n
          leftDenominatorTerm += m * m
          rightDenominatorTerm += n * n

        # Compute coconsumption factor
        coconsumed_item_count = len(intersection)
        coconsumption_ratio = coconsumed_item_count / len(test_user_train_films)

        # Aggregate contributions
        denominator = math.sqrt(leftDenominatorTerm * rightDenominatorTerm)
        if denominator == 0: # Div by zero fallback: similarity set to zero
          #list_of_similarities.append(0.0)
          pass
        else:
          similarity = float(numerator) / denominator 
          similarity *= lerp(coconsumption_factor, 0.0, 1.0, 1.0, coconsumption_ratio)

          #list_of_similarities.append(similarity)

          if abs(similarity) > similarity_threshold:
            sufficiently_similar_others.append((train_user, similarity))

      #user_mean_similarities.append(sum(list_of_similarities) / len(list_of_similarities))  # Data exploration
      #print(" Similarity threshold of {} excluded {} of {} training users".format(similarity_threshold, len(sufficiently_similar_others), len(users) - k_folder.fold_length), end = '')


      # 4) Make predictions for the test user's test films
      user_mse = 0
      user_mae = 0
      for test_film in test_user_test_films:
        # For every test film in the test film set

        # Predict rating
        numerator = 0
        denominator = 0
        #for train_user, similarity in zip(k_folder.train_users_iterator(), list_of_similarities):
        for train_user, similarity in sufficiently_similar_others:
          # For every training user, add that user's contribution to perturbation from average

          if test_film not in train_user:
            # If training user hasn't seen the film, they do not contribute to its perturbation.
            continue 

          numerator += similarity * (train_user[test_film]  - train_user_avg_rating)
          denominator += abs(similarity) # TODO
        
        # Div by zero fallback: set perturbation to zero; prediction will be average rating
        predicted_rating = test_user_avg_rating + (0.0 if denominator == 0 else (numerator / denominator))
        
        # Alias ground truth rating for clarity
        actual_rating = test_user_test_films[test_film] 
        
        #Comparison predicted rating with actual rating
        squared_error = (predicted_rating - actual_rating)**2
        user_mse += squared_error
        absolute_error = abs(predicted_rating - actual_rating)
        user_mae += absolute_error
      
      # 5) Aggregate per-film scores to get per-test-user score
      user_mse /= len(test_user_test_films)
      user_mae /= len(test_user_test_films)

      # 6) Aggregate per-user scores to get fold score
      fold_mse += user_mse
      fold_mae += user_mae

    print("") # cut off per-fold progress bar printing

    # Get and report fold mean squared error
    fold_mse /= k_folder.fold_length
    fold_mae /= k_folder.fold_length
    print("Fold MSE: {}, Fold MAE: {}".format(fold_mse, fold_mae))
    overall_mse += fold_mse
    overall_mae += fold_mae

    # Report user mean similarities (data exploration)
    #plt.hist(user_mean_similarities)
    #plt.xlabel('Mean Pearson correlation per-reviewer for all other reviewers')
    #plt.title('Histogram of mean inter-reviewer similarity')
    #plt.show()
    
  overall_mse /= k
  overall_mae /= k
  return (overall_mse, overall_mae)

"""#### Hyperparameter Search"""

from numpy.random import uniform

# Homebrew Randomized Hyperparameter Search
similarity_threshold_distribution = (0.0, 1.0)
coconsumption_factor_distribution = (0.0, 1.0)
controversy_factor_distribution = (0.0, 3.0)

num_trials = 3
k = 3

best_simil = None
best_cocon = None
best_controv = None
best_mse = None
best_mae = None
for test_simil, test_cocon, test_controv in zip(uniform(similarity_threshold_distribution[0], similarity_threshold_distribution[1], num_trials), uniform(coconsumption_factor_distribution[0], coconsumption_factor_distribution[1], num_trials), uniform(controversy_factor_distribution[0], controversy_factor_distribution[1], num_trials)):
  print("Trialling Simil: {} Cocon: {} Controv: {}".format(test_simil, test_cocon, test_controv))
  test_mse, test_mae = train_test_nn_cf(similarity_threshold = test_simil, coconsumption_factor=test_cocon, controversy_factor=test_controv, k = 3)
  print("Overall Result MSE: {}, MAE: {}".format(test_mse, test_mae))
  if best_mse is None or best_mse > test_mse:
    best_mse = test_mse
    best_mae = test_mae
    best_simil = test_simil
    best_cocon = test_cocon
    best_controv = test_controv
print("\n TRIALS COMPLETE. Best Model Found: Simil: {} Cocon: {} Controv: {}\nMSE: {} MAE: {}".format(best_simil, best_cocon, best_controv, best_mse, best_mae))

"""#### Final Trial"""

best_mse, best_mae = train_test_nn_cf(similarity_threshold = 0.97, coconsumption_factor=0.88, controversy_factor=1.85, k = 10)
print("Overall MSE: {}, Overall MAE: {}".format(best_mse, best_mae))