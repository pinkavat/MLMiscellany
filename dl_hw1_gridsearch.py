# -*- coding: utf-8 -*-
"""HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V6zfu4nQSZgq4Mv64PH9MPIpwmkpgZrh

# Colab Notebook for DL Homework 1
Written April 2023 by Thomas Pinkava
"""

# Mount drive for reading...
from google.colab import drive
drive.mount('/content/drive')

# ...and import USPS dataset
drive_file_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/USPS_all.mat'
import scipy.io
mat_dict = scipy.io.loadmat(drive_file_path)
input_X = mat_dict['fea'] # The featureset; data consists of 16x16 grayscale images
input_y = mat_dict['gnd'] # Integer labels, from 1 to 10

# Move the class labels one down so that they index properly
input_y = [int(y - 1) for y in input_y]

# One-hot encode target labels
from keras.utils import to_categorical
input_y = to_categorical(input_y, num_classes=10)

# Hold back a small sample for evaluating performance of best found model
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(input_X, input_y, test_size=0.1)

# Rebalance classes by undersampling ("1" and "2" appear about twice as often as lesser-represented classes!)
from imblearn.under_sampling import RandomUnderSampler
undersampler = RandomUnderSampler(random_state=1)
X, y = undersampler.fit_resample(X_train, y_train)

# Hyperparameter search space constructor: assembles the dictionary to
# pass to the Grid Search.
import keras.optimizers

def construct_search_grid():

  # Construct possible layer assignments
  layer_specs = []

  # Enumerate values to search over
  activations = ["relu", "sigmoid", "tanh"]
  kernel_initializers = ["random_normal", "random_uniform"] # TODO readd ones
  bias_initializers = ["random_normal"]                     # TODO readd ones
  # In order to avoid combinatorial explosion, all layers will use the same
  # activation (except for the output, which will be softmax) and initializers

  # Found some rules of thumb lying around StackOverflow that suggest that
  # good neuron counts are between the input and output size, or
  # 2/3 the size of the input plus the output size, or less than twice the
  # size of the input layer. So we have a lower bound of 181, and an upper bound
  # of 512, going by these heuristics.
  units = [x for x in range(181, 512, 100)]
  # Likewise, to avoid combinatorial explosion, both hidden layers (if two)
  # will be the same size; otherwise GridSearch is intractably large.
  
  # See above: the layers share spec, to avoid brobdignagian search space
  combinations = 0
  for kernel_initializer in kernel_initializers:
    for bias_initializer in bias_initializers:
        for activation in activations:
          for unit_count in units:
            combinations += 2
            layer = {"units":unit_count, "activation":activation, "kernel_initializer":kernel_initializer, "bias_initializer":bias_initializer}
            # Add one-layer spec
            layer_specs.append([layer])
            # Add two-layer spec
            layer_specs.append([layer, layer])
  print(f"{combinations} combinations of layer configurations to be tested")

  # Prepare candidate training schemas, with varying subparameters (LR, Momentum where appropriate)
  # Algorithms chosen from keras' bin of possibilities
  optimizers = []

  # Adadelta (cut due to combinatorial excess)
  #for learning_rate in [0.1**float(x) for x in range(1,4)]:
  #  optimizers.append(keras.optimizers.Adadelta(learning_rate = learning_rate))

  # Adam
  for learning_rate in [0.1**float(x) for x in range(1,4)]:
    optimizers.append(keras.optimizers.Adam(learning_rate = learning_rate))

  # RMSProp and SGD
  for learning_rate in [0.1**float(x) for x in range(1,3)]:
    for momentum in [0.0, 0.9]:
      optimizers.append(keras.optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum))
      optimizers.append(keras.optimizers.SGD(learning_rate = learning_rate, momentum = momentum))


  return dict(
    # Training parameters, passed to the KerasClassifier Object
    epochs = [1, 5, 10],

    # Construction parameters, passed to the create_model function
    model__optimizer = optimizers,
    model__loss = ['categorical_crossentropy'],
    model__layer_specs = layer_specs,
  )

#!python -m pip install scikeras  # Install KerasClassifier wrapper if necessary

import numpy as np
from sklearn.model_selection import GridSearchCV
from scikeras.wrappers import KerasClassifier
from keras.models import Sequential
from keras.layers import Dense

# KerasClassifier expects a function to call to create its models.
# Apparently any parameter in the CV grid prefixed with "model__" will be 
# passed to the equivalent unprefixed parameter in the create_model call.
# Not quite as horrifying as Ruby's automatic pluralization, but close.
def create_model(optimizer=None, loss=None, layer_specs=None):
  model = Sequential()

  # Add hidden layers to model, per provided spec
  # Pop layer spec to get spec of first layer, as this needs to be told the input shape
  first_layer_spec = layer_specs.pop(0)
  model.add(Dense(units=first_layer_spec["units"], activation=first_layer_spec["activation"], kernel_initializer=first_layer_spec["kernel_initializer"],bias_initializer=first_layer_spec["bias_initializer"], input_shape=(X.shape[1],)))

  # Add remaining hidden layers
  for layer_spec in layer_specs:
    model.add(Dense(units=layer_spec["units"], activation=layer_spec["activation"], kernel_initializer=layer_spec["kernel_initializer"], bias_initializer=layer_spec["bias_initializer"]))

  # Add output layer
  model.add(Dense(units=len(y[0]), activation='softmax'))

  # Compile model and return
  model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])
  return model


# Create the classifier template
clf = KerasClassifier(model = create_model, verbose = 0)

# Assemble the hyperparameter search space
hyperparameter_grid = construct_search_grid()

# Perform cross-validated grid search
grid_search = GridSearchCV(
    estimator = clf,
    param_grid = hyperparameter_grid,
    cv = 3,                             # Arbitrarily 3-fold, to keep per-spec training costs low
    verbose = 3,                        # Print diagnostics
    scoring = 'f1_micro',               # F1 as specified, arbitrarily micro (try macro?)
    refit = True                        # Refit so time can be computed
)

# Get best estimator from search results
search_result = grid_search.fit(X, y)
best_clf = search_result.best_estimator_
best_params = search_result.best_params_
best_train_time = search_result.refit_time_
print("Training Complete, best F1 score: {}".format(search_result.best_score_))
print(search_result.best_estimator_.model_.summary())

# Output the best model's architecture and training time
print("Best parameters: {}".format(best_params))
print("Trained in {} seconds".format(best_train_time))

# Use withheld test data to evaluate model performance
y_pred = best_clf.predict(X_test)
y_probs = best_clf.predict_proba(X_test)

# Convert to single-class labels
y_pred = [np.argmax(y) for y in y_pred]
y_test = [np.argmax(y) for y in y_test]

# Output scoring metrics for the best model
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, accuracy_score, RocCurveDisplay
from sklearn.preprocessing import LabelBinarizer

# General Performance Metrics
print("Micro-averaged F1 Score: {}".format(f1_score(y_test, y_pred, average='micro')))
print("Macro-averaged F1 Score: {}".format(f1_score(y_test, y_pred, average='macro')))
print("Accuracy: {}".format(accuracy_score(y_test, y_pred)))

# Confusion matrix
confusion_matrix = confusion_matrix(y_test, y_pred, labels=best_clf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=best_clf.classes_)
disp.plot()
plt.title("Confusion Matrix")
plt.show()

# ROC Plot(s)
# Boilerplate from docs at https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html
label_binarizer = LabelBinarizer().fit(y) # Obtain class labels
binarized_y = label_binarizer.transform(y_test)

RocCurveDisplay.from_predictions(
    binarized_y.ravel(),
    y_probs.ravel(),
    name="micro-avg o-v-a",
    color="darkorange",
)
plt.plot([0, 1], [0, 1], "k--")
plt.axis("square")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Micro-avg one-vs-all ROC")
plt.legend()
plt.show()

best_params['model__optimizer'].momentum